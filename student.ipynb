{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Student name: Vinayak Modgil \n",
    "- Student pace: self paced / part time / full time: Full Time\n",
    "- Scheduled project review date/time:\n",
    "- Instructor name: Yish Lim\n",
    "- Blog post URL:\n",
    "- Video of 5-min Non-Technical Presentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Introduction](#Introduction)\n",
    "- [Data Collection](#Data-Collection)\n",
    "- [Data Cleaning](#Data-Cleaning)\n",
    "- [Data Exploration](#Data-Exploration)\n",
    "- [Data Modeling](#Data-Modeling)\n",
    "- [Data Interpretation](#Data-Interpretation)\n",
    "- [Recommendations and Conclusions](#Recommendations-and-Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crash data shows information about each traffic crash on city streets within the City of Chicago limits and under the jurisdiction of Chicago Police Department (CPD). Data are shown as is from the electronic crash reporting system (E-Crash) at CPD, excluding any personally identifiable information. A dataset housing this information can be found [here](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Statement\n",
    "It is very crucial for the Vehicle Safety Board to determine the cause of an accident. With this paricular dataset, the city of Chicago has been chosen for the analysis of the accidents occuring in the city. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Methodology\n",
    "\n",
    "The dataset has information on about 520,000 car crashes in the city of Chicago, for which about 60% have a known contributory cause. Information on these craashes include many important factors that led to the crashes and the aftermath of the crashes.I will clean and explore the data to be utilized with a clasification machine learning model to predict the most known contributory cause.\n",
    "\n",
    "More specifically, I will dive deep into exploring and tuning the models so that the best known contributory cause can be known. From there, I will make predictions and conclusions which will finally lead to the most prevailing cause of an accident occuring in the city of Chicago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data wrangling and visualization packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "\n",
    "#feature engineering packages\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "#feature selection packages\n",
    "from feature_engine.selection import DropDuplicateFeatures\n",
    "\n",
    "#modeling packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#modeling evaluation packages\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_roc_curve, roc_curve, auc\n",
    "from sklearn.metrics import get_scorer\n",
    "\n",
    "#optimization packages\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notebook settings\n",
    "pd.set_option(\"display.max_columns\", 40)\n",
    "pd.options.display.float_format = '{:,}'.format\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "impute_mean = SimpleImputer(strategy = \"mean\")\n",
    "impute_median = SimpleImputer(strategy = \"median\")\n",
    "impute_mode = SimpleImputer(strategy = \"most_frequent\")\n",
    "impute_cont_const = SimpleImputer(strategy = \"constant\", fill_value = 0)\n",
    "impute_cat_const = SimpleImputer(strategy = \"constant\", fill_value= \"missing\")\n",
    "\n",
    "\n",
    "def clean_df(df):\n",
    "    '''\n",
    "    Takes dataset df as input and returns a clean dataset \n",
    "    with null values taken care of.\n",
    "    '''\n",
    "    # Dividing datasets in continuous and catergorical variables\n",
    "    cont_features = [col for col in df.columns if df[col].dtype in [np.float64, np.int64]]\n",
    "    cat_features = [col for col in df.columns if df[col].dtype in [np.object]]\n",
    "    \n",
    "    \n",
    "    #filling injuries continuous variables with mean\n",
    "    injuries = [\"INJURIES_TOTAL\", \"INJURIES_FATAL\", \"INJURIES_INCAPACITATING\", \"INJURIES_NON_INCAPACITATING\"\n",
    "               , \"INJURIES_REPORTED_NOT_EVIDENT\", \"INJURIES_NO_INDICATION\", \"INJURIES_UNKNOWN\"]\n",
    "    \n",
    "    df[injuries] = impute_mean.fit_transform(df[injuries])\n",
    "    \n",
    "    # filling latitude and longitude continuous variables with 0\n",
    "    lat_long = [\"LATITUDE\", \"LONGITUDE\"]\n",
    "    \n",
    "    df[lat_long] = impute_cont_const.fit_transform(df[lat_long])\n",
    "    \n",
    "    #filling beat of occurrence continuous variable with median\n",
    "    beat_of_occ = [\"BEAT_OF_OCCURRENCE\"]\n",
    "    df[beat_of_occ] = impute_median.fit_transform(df[beat_of_occ])\n",
    "    \n",
    "    # Filling null categorical values with \"missing\"\n",
    "    cat_vars = [\"RD_NO\", \"CRASH_DATE_EST_I\", \"LANE_CNT\", \"REPORT_TYPE\", \"INTERSECTION_RELATED_I\",\n",
    "               \"NOT_RIGHT_OF_WAY_I\", \"HIT_AND_RUN_I\", \"PHOTOS_TAKEN_I\", \"STATEMENTS_TAKEN_I\", \"DOORING_I\", \"WORK_ZONE_I\",\n",
    "               \"WORK_ZONE_TYPE\", \"WORKERS_PRESENT_I\", \"MOST_SEVERE_INJURY\", \"LOCATION\", \"STREET_DIRECTION\", \"STREET_NAME\"]\n",
    "    \n",
    "    df[cat_vars] = impute_cat_const.fit_transform(df[cat_vars])    \n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_to_drop(df, y=None):\n",
    "    '''\n",
    "    Cleans rows which are not needed\n",
    "    '''\n",
    "    if y!= None:\n",
    "        df_with_index = df.set_index(y)\n",
    "        \n",
    "        df_with_index.drop(labels=[\"UNABLE TO DETERMINE\", \"NOT APPLICABLE\"], axis=0, inplace=True)\n",
    "        df_with_index.reset_index(inplace=True)\n",
    "    return df_with_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_to_drop_unknown(df, y=None):\n",
    "    '''\n",
    "    Cleans rows which are not needed\n",
    "    '''\n",
    "    if y!= None:\n",
    "        df_with_index = df.set_index(y)\n",
    "        \n",
    "        df_with_index.drop(labels=[\"UNKNOWN\"], axis=0, inplace=True)\n",
    "        df_with_index.reset_index(inplace=True)\n",
    "    return df_with_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_quasi_const(df):\n",
    "    '''\n",
    "    Function taken from Feature Engineering course on Udemy to drop all\n",
    "    the constant and quasi-constant features.\n",
    "    - df: A dataframe\n",
    "    '''\n",
    "    #Create an empty list\n",
    "    quasi_const_feat = []\n",
    "    \n",
    "    #Iterate over every feature\n",
    "    for feature in df.columns:\n",
    "        \n",
    "        #Find the predominant value, the value that is \n",
    "        # shared by most observations\n",
    "        predominant = (df[feature].value_counts() /\n",
    "                       np.float(len(df))).sort_values(ascending=False).values[0]\n",
    "        \n",
    "        #Evaluate the predominant feature: do more than 99% of the observations\n",
    "        #show 1 value?\n",
    "        if predominant > 0.998:\n",
    "            \n",
    "            #if yes, append it to the empt list\n",
    "            quasi_const_feat.append(feature)\n",
    "            \n",
    "    df.drop(labels=quasi_const_feat, axis=1, inplace=True)\n",
    "    return df\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_summary(df, num_col=None, cat_cols=None, y_col = \"PRIM_CONTRIBUTORY_CAUSE\", label_count = 25, thresh = 0.025):\n",
    "    '''\n",
    "    this function gives a brief summary of a single col \n",
    "    in the dataset df. Also, it shows the essential plots\n",
    "    required for the column w.r.t the dependent variable.\n",
    "    \n",
    "    arguments:\n",
    "    df - given dataset\n",
    "    num_col - numerical column in the dataset\n",
    "    cat_cols - categorical columns in the dataset\n",
    "    y_col - dependent variable\n",
    "    label_count - number of labels to draw in bar graph\n",
    "    '''\n",
    "    if num_col != None:\n",
    "        #print the column name\n",
    "        print(f'Column Name: {num_col}') \n",
    "        #print the number of unique values\n",
    "        print(f'Number of unique values: {df[num_col].nunique()}') \n",
    "        #print the number of duplicate values\n",
    "        print(f'There are {df[num_col].duplicated().sum()} duplicates')\n",
    "        #print the number of null values\n",
    "        print(f'There are {df[num_col].isna().sum()} null values')\n",
    "        #print the number of values equal to 0\n",
    "        print(f'There are {(df[num_col] == 0).sum()} zeros')\n",
    "        print('\\n')\n",
    "        #print the value counts percentage\n",
    "        print('Value Counts Percentage', '\\n', \n",
    "              df[num_col].value_counts(normalize=True, dropna=False).round(2)*100)\n",
    "        print('\\n')\n",
    "        #print descriptive statistics\n",
    "        print('Descriptive Metrics:','\\n',\n",
    "              df[num_col].describe())\n",
    "        #plot boxplot, histogram         \n",
    "        fig, ax = plt.subplots(nrows=4, figsize=(15,80))\n",
    "        \n",
    "        histogram = df[num_col].hist(ax=ax[0])\n",
    "        ax[0].set_title(f'Distribution of {num_col}');\n",
    "        \n",
    "        scatter = df.plot(kind='scatter', x=num_col, y=y_col,ax=ax[1]);\n",
    "        ax[1].set_title(f'{y_col} vs {num_col}');\n",
    "\n",
    "        boxplot = df.boxplot(column=num_col, ax=ax[2]);\n",
    "        ax[2].set_title(f'Boxplot of {num_col}');\n",
    "\n",
    "        sm.graphics.qqplot(df[num_col], dist=stats.norm, line='45', fit=True, ax=ax[3])\n",
    "        ax[3].set_title(f'QQ plot of {num_col}');\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for col in cat_cols:\n",
    "            print('============================')\n",
    "            #print the column name\n",
    "            print(f'Column Name: {col}')\n",
    "            print('\\n')\n",
    "            #print the number of unique values\n",
    "            print(f'Number of unique values: {df[col].nunique()}')\n",
    "            print('\\n')\n",
    "            #print the number of duplicate values\n",
    "            print(f'There are {df[col].duplicated().sum()} duplicates')\n",
    "            print('\\n')\n",
    "            #print the number of null values\n",
    "            print(f'There are {df[col].isna().sum()} null values')\n",
    "            print('\\n')\n",
    "            #print the number of values equal to '0'\n",
    "            print(f'There are {(df[col] == \"0\").sum()} zeros')\n",
    "            print('\\n')\n",
    "            #print the value counts percentage\n",
    "            print('Value Counts Percentage', '\\n', \n",
    "                  df[col].value_counts(dropna=False).round(2))\n",
    "            print('\\n')\n",
    "\n",
    "            #plot barplot, histogram     \n",
    "            fig, ax = plt.subplots(figsize=(15,10))\n",
    "                        \n",
    "            bar_graph = df[col].value_counts(normalize=True, \n",
    "                                             dropna=False)[:label_count].plot.bar(label=f'{col} Percentage')\n",
    "            ax.axhline(y=thresh, color='red', linestyle='--', \n",
    "                        label=f'{thresh*100}% Threshold')\n",
    "            ax.set_title(f'{col} Value Counts')\n",
    "            ax.set_xlabel(f'{col} Labels')\n",
    "            ax.set_ylabel('Percentage')\n",
    "            ax.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(y_true, y_pred):\n",
    "    #Create an instance of confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    #Plot it on a heatmap \n",
    "    sns.heatmap(cm, annot=True, fmt=\"0.2g\", cmap = sns.color_palette(\"Blues\"))\n",
    "    print\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model,X_train, X_test, y_train, y_test, prev_model=None):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cars = pd.read_csv(\"data/Traffic_Crashes_-_Vehicles.csv\")\n",
    "df_crashes = pd.read_csv(\"data/Traffic_Crashes_-_Crashes.csv\")\n",
    "df_crashes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Schema\n",
    "\n",
    "**Taken From:** [Chicago car crash website](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if)\n",
    "- `CRASH_RECORD_ID`\n",
    "This number can be used to link to the same crash in the Vehicles and People datasets. This number also serves as a unique ID in this dataset.\n",
    "\n",
    "- `RD_NO`\n",
    "Chicago Police Department report number. For privacy reasons, this column is blank for recent crashes.\n",
    "\n",
    "- `CRASH_DATE_EST_I`\t\n",
    "Crash date estimated by desk officer or reporting party (only used in cases where crash is reported at police station days after the crash)\n",
    "\n",
    "- `CRASH_DATE`\t\n",
    "Date and time of crash as entered by the reporting officer\n",
    "\n",
    "- `POSTED_SPEED_LIMIT`\t\n",
    "Posted speed limit, as determined by reporting officer\n",
    "\n",
    "- `TRAFFIC_CONTROL_DEVICE`\t\n",
    "Traffic control device present at crash location, as determined by reporting officer\n",
    "\n",
    "- `DEVICE_CONDITION`\t\n",
    "Condition of traffic control device, as determined by reporting officer\n",
    "\n",
    "- `WEATHER_CONDITION`\t\n",
    "Weather condition at time of crash, as determined by reporting officer\n",
    "\n",
    "- `LIGHTING_CONDITION`\t\n",
    "Light condition at time of crash, as determined by reporting officer\n",
    "\n",
    "- `FIRST_CRASH_TYPE`\t\n",
    "Type of first collision in crash\n",
    "\n",
    "- `TRAFFICWAY_TYPE`\t\n",
    "Trafficway type, as determined by reporting officer\n",
    "\n",
    "- `LANE_CNT`\t\n",
    "Total number of through lanes in either direction, excluding turn lanes, as determined by reporting officer (0 = intersection)\n",
    "\n",
    "- `ALIGNMENT`\t\n",
    "Street alignment at crash location, as determined by reporting officer\n",
    "\n",
    "- `ROADWAY_SURFACE_COND`\t\n",
    "Road surface condition, as determined by reporting officer\n",
    "\n",
    "- `ROAD_DEFECT`\t\n",
    "Road defects, as determined by reporting officer\n",
    "\n",
    "- `REPORT_TYPE`\t\n",
    "Administrative report type (at scene, at desk, amended)\n",
    "\n",
    "- `CRASH_TYPE`\t\n",
    "A general severity classification for the crash. Can be either Injury and/or Tow Due to Crash or No Injury / Drive Away\n",
    "\n",
    "- `INTERSECTION_RELATED_I`\t\n",
    "A field observation by the police officer whether an intersection played a role in the crash. Does not represent whether or not the crash occurred within the intersection.\n",
    "\n",
    "- `NOT_RIGHT_OF_WAY_I`\t\n",
    "Whether the crash begun or first contact was made outside of the public right-of-way.\n",
    "\n",
    "- `HIT_AND_RUN_I`\t\n",
    "Crash did/did not involve a driver who caused the crash and fled the scene without exchanging information and/or rendering aid\n",
    "\n",
    "- `DAMAGE`\t\n",
    "A field observation of estimated damage.\n",
    "\n",
    "- `DATE_POLICE_NOTIFIED`\t\n",
    "Calendar date on which police were notified of the crash\n",
    "\n",
    "- `PRIM_CONTRIBUTORY_CAUSE`\t\n",
    "The factor which was most significant in causing the crash, as determined by officer judgment\n",
    "\n",
    "- `SEC_CONTRIBUTORY_CAUSE`\t\n",
    "The factor which was second most significant in causing the crash, as determined by officer judgment\n",
    "\n",
    "- `STREET_NO`\t\n",
    "Street address number of crash location, as determined by reporting officer\n",
    "\n",
    "- `STREET_DIRECTION`\t\n",
    "Street address direction (N,E,S,W) of crash location, as determined by reporting officer\n",
    "\n",
    "- `STREET_NAME`\t\n",
    "Street address name of crash location, as determined by reporting officer\n",
    "\n",
    "- `BEAT_OF_OCCURRENCE`\t\n",
    "Chicago Police Department Beat ID. Boundaries available at https://data.cityofchicago.org/d/aerh-rz74\n",
    "\n",
    "- `PHOTOS_TAKEN_I`\t\n",
    "Whether the Chicago Police Department took photos at the location of the crash\n",
    "\n",
    "- `STATEMENTS_TAKEN_I`\t\n",
    "Whether statements were taken from unit(s) involved in crash\n",
    "\n",
    "- `DOORING_I`\t\n",
    "Whether crash involved a motor vehicle occupant opening a door into the travel path of a bicyclist, causing a crash\n",
    "\n",
    "- `WORK_ZONE_I`\t\n",
    "Whether the crash occurred in an active work zone\n",
    "\n",
    "- `WORK_ZONE_TYPE`\t\n",
    "The type of work zone, if any\n",
    "\n",
    "- `WORKERS_PRESENT_I`\t\n",
    "Whether construction workers were present in an active work zone at crash location\n",
    "\n",
    "- `NUM_UNITS`\t\n",
    "Number of units involved in the crash. A unit can be a motor vehicle, a pedestrian, a bicyclist, or another non-passenger roadway user. Each unit represents a mode of traffic with an independent trajectory.\n",
    "\n",
    "- `MOST_SEVERE_INJURY`\t\n",
    "Most severe injury sustained by any person involved in the crash\n",
    "\n",
    "- `INJURIES_TOTAL`\t\n",
    "Total persons sustaining fatal, incapacitating, non-incapacitating, and possible injuries as determined by the reporting officer\n",
    "\n",
    "- `INJURIES_FATAL`\t\n",
    "Total persons sustaining fatal injuries in the crash\n",
    "\n",
    "- `INJURIES_INCAPACITATING`\t\n",
    "Total persons sustaining incapacitating/serious injuries in the crash as determined by the reporting officer. Any injury other than fatal injury, which prevents the injured person from walking, driving, or normally continuing the activities they were capable of performing before the injury occurred. Includes severe lacerations, broken limbs, skull or chest injuries, and abdominal injuries.\n",
    "\n",
    "- `INJURIES_NON_INCAPACITATING`\t\n",
    "Total persons sustaining non-incapacitating injuries in the crash as determined by the reporting officer. Any injury, other than fatal or incapacitating injury, which is evident to observers at the scene of the crash. Includes lump on head, abrasions, bruises, and minor lacerations.\n",
    "\n",
    "- `INJURIES_REPORTED_NOT_EVIDENT`\t\n",
    "Total persons sustaining possible injuries in the crash as determined by the reporting officer. Includes momentary unconsciousness, claims of injuries not evident, limping, complaint of pain, nausea, and hysteria.\n",
    "\n",
    "- `INJURIES_NO_INDICATION`\t\n",
    "Total persons sustaining no injuries in the crash as determined by the reporting officer\n",
    "\n",
    "- `INJURIES_UNKNOWN`\t\n",
    "Total persons for whom injuries sustained, if any, are unknown\n",
    "\n",
    "- `CRASH_HOUR`\t\n",
    "The hour of the day component of CRASH_DATE.\n",
    "\n",
    "- `CRASH_DAY_OF_WEEK`\t\n",
    "The day of the week component of CRASH_DATE. Sunday=1\n",
    "\n",
    "- `CRASH_MONTH`\t\n",
    "The month component of CRASH_DATE.\n",
    "\n",
    "- `LATITUDE`\t\n",
    "The latitude of the crash location, as determined by reporting officer, as derived from the reported address of crash\n",
    "\n",
    "- `LONGITUDE`\t\n",
    "The longitude of the crash location, as determined by reporting officer, as derived from the reported address of crash\n",
    "\n",
    "- `LOCATION`\t\n",
    "The crash location, as determined by reporting officer, as derived from the reported address of crash, in a column type that allows for mapping and other geographic analysis in the data portal software\n",
    "Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - Many columns to explore for null value imputation\n",
    "> - Column names are already standardized\n",
    "> - Data types will require further evaluation during engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate numerical data descriptive statistics\n",
    "df_crashes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Observations**\n",
    "> - Few of these numerical features should be transformed into a categorical feature.\n",
    "> - `INJURIES_TOTAL`, `INJURIES_FATAL`, `INJURIES_INCAPACITATING`, `INJURIES_NON_INCAPACITATING`, `INJURIES_REPORTED_NOT_EVIDENT`, `INJURIES_NO_INDICATION`, `INJURIES_UNKNOWN`, `CRASH_HOUR` has a minimumum of 0 which may be placeholder for unknown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes_clean = df_crashes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using the global function clean_df to impute null values\n",
    "df_crashes_clean = clean_df(df_crashes_clean)\n",
    "df_crashes_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of all columns\n",
    "num_cols = [col for col in df_crashes_clean.columns if df_crashes_clean[col].dtype in [np.float64, np.int64]]\n",
    "cat_cols= [col for col in df_crashes_clean.columns if df_crashes_clean[col].dtype in [np.object]]\n",
    "print(f\"There are {len(num_cols)} numerical columns : \\n {num_cols}\")\n",
    "print(\"\\n\")\n",
    "print(f\"There are {len(cat_cols)} categorical columns : \\n {cat_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows of numeric columns\n",
    "df_crashes_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# posted speed limit summary\n",
    "col_summary(df_crashes_clean, num_col=\"POSTED_SPEED_LIMIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - Does not seem to have extreme outliers\n",
    "\n",
    "> **Actions**\n",
    "> - Keep all the values in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Street no summary\n",
    "col_summary(df_crashes_clean, num_col=\"STREET_NO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - `STREET_NO` should be changed to a categorical variable as it is a unique identifier.\n",
    "\n",
    "> **Actions**\n",
    "> - Recast `STREET_NO` as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of BEAT_OF_OCCURRENCE\n",
    "col_summary(df_crashes_clean, num_col=\"BEAT_OF_OCCURRENCE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - Needs to be changed to a categorical variable as it is an identifier.\n",
    "> **Actions**\n",
    "> - Recast `BEAT_OF_OCCURRENCE` as a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of NUM_UNITS\n",
    "col_summary(df_crashes_clean, num_col = \"NUM_UNITS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - There are outliers present here.\n",
    "\n",
    "> **Actions**\n",
    "> - Remove outliers from `NUM_UNITS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, num_col=\"INJURIES_TOTAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - Maybe useful for modeling by engineering features\n",
    "\n",
    "> **Actions**\n",
    "> - Keep the column `INJURIES_TOTAL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, num_col=\"INJURIES_FATAL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - Seems to be useful for modeling\n",
    "\n",
    "> **Actions**\n",
    "> - Keep the column `INJURIES_FATAL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, num_col=\"INJURIES_INCAPACITATING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - `INJURIES INCAPACITATING` has many zeros and needs to evaluated for outliers.\n",
    "\n",
    "> **Actions**\n",
    "> - Check for outliers in the column.\n",
    "> - Keep the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, num_col=\"INJURIES_NON_INCAPACITATING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - Doesnt seem useful for modeling\n",
    "\n",
    "> **Actions**\n",
    "> - Drop the column `INJURIES_NON_INCAPACITATING`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, num_col=\"INJURIES_REPORTED_NOT_EVIDENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> about 95% of the values are 0 and the data schema does not clearly state what it means, I will drop this column from analysis.\n",
    "\n",
    "> **Actions**\n",
    "> - Drop `INJURIES_REPORTED_NOT_EVIDENT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, num_col=\"INJURIES_NO_INDICATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - Column seems to useful for classification\n",
    "\n",
    "> **Actions**\n",
    "> - Keep the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, num_col=\"INJURIES_UNKNOWN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - Doesnt seem to be a useful column\n",
    "\n",
    "> **Actions**\n",
    "> - Drop `INJURIES_UNKNOWN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, num_col=\"CRASH_HOUR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - Seems useful for modeling\n",
    "\n",
    "> **Actions**\n",
    "> - Keep the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, num_col=\"CRASH_DAY_OF_WEEK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - Doesnt seem to be useful\n",
    "\n",
    "> **Actions**\n",
    "> - Drop `CRASH_DAY_OF_WEEK`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, num_col=\"CRASH_MONTH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - Doesnt seem to be useful\n",
    "\n",
    "> **Actions**\n",
    "> - Drop `CRASH_MONTH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, num_col=\"LATITUDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observations**\n",
    "> - Latitude should be a categorical as it is an identifier\n",
    "\n",
    "> **Actions**\n",
    "> - Recast `LATITUDE` as a categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, num_col=\"LONGITUDE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **OBSERVATION**\n",
    "> - `LONGITUDE` seems to be a categorical variable as it is an identifier.\n",
    "\n",
    "> **Action**\n",
    "> - Recast `LONGITUDE` as a categorical variable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, cat_cols = [\"CRASH_RECORD_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, cat_cols = [\"RD_NO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, cat_cols = [\"CRASH_DATE\", \"CRASH_DATE_EST_I\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, cat_cols=[\"TRAFFIC_CONTROL_DEVICE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, cat_cols=[\"DEVICE_CONDITION\", \"WEATHER_CONDITION\", \"LIGHTING_CONDITION\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, cat_cols=['FIRST_CRASH_TYPE',\n",
    " 'TRAFFICWAY_TYPE','REPORT_TYPE', 'CRASH_TYPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " col_summary(df_crashes_clean, cat_cols=['LANE_CNT', 'ALIGNMENT', 'ROADWAY_SURFACE_COND',\n",
    " 'ROAD_DEFECT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, cat_cols=['INTERSECTION_RELATED_I',\n",
    " 'NOT_RIGHT_OF_WAY_I', 'HIT_AND_RUN_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, cat_cols=['DAMAGE', 'DATE_POLICE_NOTIFIED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, cat_cols=[\"PRIM_CONTRIBUTORY_CAUSE\", \"SEC_CONTRIBUTORY_CAUSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, cat_cols=['STREET_DIRECTION', 'STREET_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, cat_cols=['PHOTOS_TAKEN_I', 'STATEMENTS_TAKEN_I', 'DOORING_I', 'WORK_ZONE_I', \n",
    " 'WORK_ZONE_TYPE', 'WORKERS_PRESENT_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, cat_cols=[\"MOST_SEVERE_INJURY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_summary(df_crashes_clean, cat_cols=[\"LOCATION\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------\n",
    "**Feature evaluation done**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Summary of actions to take**\n",
    ">- recast `STREET_NO` as a string\n",
    ">- recast  `BEAT_OF_OCCURRENCE` as a string\n",
    ">- recast `LATITUDE` as a string\n",
    ">- recast `LONGITUDE` as a string\n",
    ">- drop `INJURIES_REPORTED_NOT_EVIDENT` column\n",
    ">- drop `INJURIES_UNKNOWN` column\n",
    ">- drop `CRASH_DAY_OF_WEEK` column\n",
    ">- drop ` CRASH_MONTH` column\n",
    ">- drop `CRASH_DATE` column\n",
    ">- drop `CRASH_RECORD_ID` column\n",
    ">- drop `INTERSECTION_RELATED_I` column\n",
    ">- drop `STREET_DIRECTION` column\n",
    ">- drop `STREET_NAME` column\n",
    ">- drop `PHOTOS_TAKEN_I` column\n",
    ">- drop `STATEMENTS_TAKEN_I` column\n",
    ">- drop `WORK_ZONE_I` column\n",
    ">- drop `WORK_ZONE_TYPE` column\n",
    ">- drop `WORKERS_PRESENT_I` column\n",
    ">- drop `LANE_CNT` column\n",
    ">- drop `ALIGNMENT` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data type Recasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert STREET_NO to categorical\n",
    "df_crashes_clean[\"STREET_NO\"] = df_crashes_clean[\"STREET_NO\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert BEAT_OF_OCCURRENCE to categorical\n",
    "df_crashes_clean[\"BEAT_OF_OCCURRENCE\"] = df_crashes_clean[\"BEAT_OF_OCCURRENCE\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert LATITUDE to categorical\n",
    "df_crashes_clean[\"LATITUDE\"] = df_crashes_clean[\"LATITUDE\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes_clean[\"LONGITUDE\"] = df_crashes_clean[\"LONGITUDE\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2e1196f531d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_crashes_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"CRASH_DATE_YR\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_crashes_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"CRASH_DATE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_crashes_clean[\"CRASH_DATE_YR\"] = pd.to_datetime(df_crashes_clean[\"CRASH_DATE\"]).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature/Row Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes_clean = df_crashes_clean.drop(columns=[\"RD_NO\",\"CRASH_DAY_OF_WEEK\", \"CRASH_MONTH\", \"CRASH_RECORD_ID\", \"INTERSECTION_RELATED_I\",\n",
    "                                            \"STREET_DIRECTION\", \"STREET_NAME\", \"PHOTOS_TAKEN_I\", \"STATEMENTS_TAKEN_I\", \n",
    "                                            \"WORK_ZONE_I\", \"WORK_ZONE_TYPE\", \"WORKERS_PRESENT_I\", \"LANE_CNT\", \"ALIGNMENT\", \"CRASH_DATE_EST_I\", \"CRASH_DATE\",\n",
    "                                                 \"NOT_RIGHT_OF_WAY_I\", \"HIT_AND_RUN_I\", \"DATE_POLICE_NOTIFIED\", \"DOORING_I\", \"INJURIES_UNKNOWN\", \"LOCATION\"], axis=1)\n",
    "df_crashes_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes_clean = rows_to_drop(df_crashes_clean,\n",
    "                                y=\"PRIM_CONTRIBUTORY_CAUSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes_clean = rows_to_drop(df_crashes_clean,\n",
    "                                y=\"SEC_CONTRIBUTORY_CAUSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes_clean = rows_to_drop_unknown(df_crashes_clean, y=\"LIGHTING_CONDITION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes_clean = drop_quasi_const(df_crashes_clean)\n",
    "df_crashes_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crashes_clean.loc[df_crashes_clean[\"NUM_UNITS\"] <= 5, [\"NUM_UNITS\"]].hist(bins=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train-test split\n",
    "X = df_crashes_clean.drop(columns=\"LIGHTING_CONDITION\")\n",
    "y = df_crashes_clean[\"LIGHTING_CONDITION\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = X_train.copy()\n",
    "X_test_tf = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I will create new features which will improve the ability to gain insights into the data and help modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SEVERELY_INJURED`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf[\"SEVERELY_INJURED\"] = X_train_tf[\"INJURIES_TOTAL\"] >= 5\n",
    "X_train_tf[\"SEVERELY_INJURED\"].value_counts()\n",
    "\n",
    "X_test_tf[\"SEVERELY_INJURED\"] = X_test_tf[\"INJURIES_TOTAL\"] >= 5\n",
    "X_test_tf[\"SEVERELY_INJURED\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data exploration df\n",
    "df_explore = pd.concat([X_train_tf, X_test_tf], axis=0)\n",
    "df_explore[\"LIGHTING_CONDITION\"] = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will take 2 major steps in preprocessing the data for modeling:\n",
    "1. Scale numerical data\n",
    "2. Encode categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training columns\n",
    "X_train_tf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf.drop(columns=[\"LATITUDE\", \"LONGITUDE\"], axis=1, inplace=True)\n",
    "X_test_tf.drop(columns=[\"LATITUDE\", \"LONGITUDE\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf.drop(\"STREET_NO\", axis=1, inplace=True)\n",
    "X_train_tf.drop(\"STREET_NO\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = X_train_tf.select_dtypes(include=\"object\").columns\n",
    "num_cols = X_train_tf.select_dtypes(exclude=\"object\").columns\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, drop=\"first\")\n",
    "ohe.fit(X_train_tf[cat_cols])\n",
    "train_ohe_df = pd.DataFrame(ohe.transform(X_train_tf[cat_cols]),\n",
    "                           columns=ohe.get_feature_names(cat_cols),\n",
    "                            index=X_train_tf.index)\n",
    "\n",
    "test_ohe_df = pd.DataFrame(ohe.transform(X_test_tf[cat_cols]),\n",
    "                           columns=ohe.get_feature_names(cat_cols),\n",
    "                           index=X_test_tf.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ohe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_tf[num_cols])\n",
    "\n",
    "train_scale_df = pd.DataFrame(scaler.transform(X_train_tf[num_cols]),\n",
    "                             columns=num_cols, index=X_train_tf.index)\\\n",
    "\n",
    "test_scale_df = pd.DataFrame(scaler.transform(X_test_tf[num_cols]),\n",
    "                             columns=num_cols, index=X_test_tf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scale_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = pd.concat([train_ohe_df, train_scale_df], axis=1)\n",
    "X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = pd.concat([test_ohe_df, test_scale_df], axis=1)\n",
    "X_test_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "First I will create a logistic regression model and check for the scores.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity with Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observation**\n",
    "> - The features seem to have a linear relationship with target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_explore.corr().abs().round(2), annot=True, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observation**\n",
    "> - High correlation between `INJURIES_TOTAL` and `INJURIES_NON_INCAPACITATING` is observed.\n",
    "\n",
    "> **Action** \n",
    ">- drop `INJURIES_NON_INCAPACITATING` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf.drop(\"INJURIES_NON_INCAPACITATING\", axis=1, inplace=True)\n",
    "X_test_tf.drop(\"INJURIES_NON_INCAPACITATING\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lr = X_train_tf.copy()\n",
    "X_test_lr = X_test_tf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1.fit(X_train_lr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1.score(X_train_lr, y_train), lr1.score(X_test_lr, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = lr1.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup = DropDuplicateFeatures(missing_values=\"raise\")\n",
    "dup.fit(X_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup.duplicated_feature_sets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-82ed9c1afcc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_test_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_lr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dup' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_lr = dup.transform(X_train_lr)\n",
    "X_test_lr = dup.transform(X_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lr.shape, X_test_lr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_2 = LogisticRegression()\n",
    "lr_2.fit(X_train_lr, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "192px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
